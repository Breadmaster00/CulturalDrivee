import numpy as np
import pandas as pd
import pickle
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
from imblearn.over_sampling import SMOTE
from config import CalculatorConfig  # –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥–∞
from data_processor import DataProcessor  # –ò–º–ø–æ—Ä—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
from nn_model import NeuralNetworkModel  # –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏
from keras.models import load_model

class PriceOptimizer:
    def __init__(self):
        self.model = None
        self.data_processor = DataProcessor()
        self.feature_importances = None  # –î–æ–±–∞–≤–ª–µ–Ω–æ
    
    def fit(self, df):
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        df_processed = self.data_processor.prepare_features(df)
        
        if df_processed is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return False, None
        
        initial_size = len(df_processed)
        df_processed = df_processed.dropna(subset=CalculatorConfig.FEATURE_COLUMNS + ['is_done'])
        final_size = len(df_processed)
        
        print(f"üìâ –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {initial_size - final_size}")
        
        if final_size == 0:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
            return False, None
        
        X = df_processed[CalculatorConfig.FEATURE_COLUMNS]
        y = df_processed['is_done']
        
        print(f"üéØ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {X.shape}")
        print(f"‚öñÔ∏è –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y.mean():.1%} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö")
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        print("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤...")
        smote = SMOTE(random_state=42)
        X, y = smote.fit_resample(X, y)
        print(f"   –ù–æ–≤—ã–π –±–∞–ª–∞–Ω—Å: {y.mean():.1%}")
        
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_train_scaled = self.data_processor.scale_features(X_train, fit=True)
        X_val_scaled = self.data_processor.scale_features(X_val, fit=False)
        
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        self.model = NeuralNetworkModel(input_dim=X_train_scaled.shape[1])
        self.model.build_model()
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        history = self.model.train(X_train_scaled, y_train, X_val_scaled, y_val)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.model.plot_training_history()
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        y_pred_proba = self.model.predict_proba(X_val_scaled)
        roc_auc = roc_auc_score(y_val, y_pred_proba)
        print(f"üìä ROC-AUC: {roc_auc:.4f}")
        
        y_pred = (np.array(y_pred_proba) > 0.5).astype(int)
        print("\nüìà –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_val, y_pred))
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á–µ—Ä–µ–∑ permutation importance)
        self._calculate_feature_importance(X_val_scaled, y_val, CalculatorConfig.FEATURE_COLUMNS)
        
        self.save_model()
        return True, {"roc_auc": float(roc_auc)}
    
    def _calculate_feature_importance(self, X_val, y_val, feature_names, n_repeats=5):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ permutation importance"""
        print("\nüîë –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        baseline_score = roc_auc_score(y_val, self.model.predict_proba(X_val))
        importances = np.zeros(len(feature_names))
        
        for i, feature_idx in enumerate(range(len(feature_names))):
            scores = []
            for _ in range(n_repeats):
                X_permuted = X_val.copy()
                np.random.shuffle(X_permuted[:, feature_idx])
                permuted_score = roc_auc_score(y_val, self.model.predict_proba(X_permuted))
                scores.append(baseline_score - permuted_score)
            
            importances[i] = np.mean(scores)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–µ–π
        if importances.sum() > 0:
            importances = importances / importances.sum()
        
        self.feature_importances = dict(zip(feature_names, importances))
        self._print_feature_importances()
    
    def _print_feature_importances(self):
        print("\nüîë –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Permutation Importance):")
        sorted_importances = sorted(
            self.feature_importances.items(),
            key=lambda x: x[1],
            reverse=True
        )
        for feature, importance in sorted_importances[:10]:
            print(f"   {feature}: {importance:.4f}")
    
    def save_model(self):
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
            if self.model and self.model.model:
                self.model.model.save(CalculatorConfig.MODEL_PATH)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            processor_data = {
                'label_encoders': self.data_processor.label_encoders,
                'price_stats': self.data_processor.price_stats,
                'feature_importances': self.feature_importances,
                'timestamp': datetime.now()
            }
            
            with open(CalculatorConfig.SCALER_PATH, 'wb') as f:
                pickle.dump({
                    'scaler': self.data_processor.scaler,
                    'is_scaler_fitted': self.data_processor.is_scaler_fitted,
                    'processor_data': processor_data
                }, f)
            
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {CalculatorConfig.MODEL_PATH}")
            print(f"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {CalculatorConfig.SCALER_PATH}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self):
        try:
            if not os.path.exists(CalculatorConfig.MODEL_PATH) or not os.path.exists(CalculatorConfig.SCALER_PATH):
                print(f"‚ùå –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
            self.model = NeuralNetworkModel(input_dim=len(CalculatorConfig.FEATURE_COLUMNS))
            self.model.model = load_model(CalculatorConfig.MODEL_PATH)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler –∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            with open(CalculatorConfig.SCALER_PATH, 'rb') as f:
                saved_data = pickle.load(f)
            
            self.data_processor.scaler = saved_data['scaler']
            self.data_processor.is_scaler_fitted = saved_data['is_scaler_fitted']
            self.data_processor.label_encoders = saved_data['processor_data']['label_encoders']
            self.data_processor.price_stats = saved_data['processor_data'].get('price_stats', {})
            self.feature_importances = saved_data['processor_data'].get('feature_importances', {})
            
            print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {CalculatorConfig.MODEL_PATH}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def predict_probability(self, order_features, bid_price):
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        feature_dict = order_features.copy()
        feature_dict['price_bid_local'] = float(bid_price)
        feature_dict['price_ratio'] = float(bid_price / feature_dict['price_start_local'] if feature_dict['price_start_local'] > 0 else 1)
        
        feature_df = pd.DataFrame([feature_dict])[CalculatorConfig.FEATURE_COLUMNS]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_scaled = self.data_processor.scale_features(feature_df)
        
        probability = self.model.predict_proba(feature_scaled)[0]
        return max(0.0, min(1.0, probability))